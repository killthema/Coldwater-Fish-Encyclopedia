import os
import io
import numpy as np
from PIL import Image
from flask import Flask, request, jsonify
from flask_cors import CORS
from tensorflow.keras.models import load_model

# ==========================================
# [ë¬´ë£Œ RAG ë¬´ê¸°ë“¤] ì˜¤í”ˆì†ŒìŠ¤ ë¡œì»¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ==========================================
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOllama


from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

app = Flask(__name__)
CORS(app)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# ==========================================
# [1] ë„ê° ëª¨ë¸ ë¡œë“œ (ê³¼ê¸ˆ ì—†ìŒ)
# ==========================================
MODEL_PATH = os.path.join(BASE_DIR, 'fish_model.h5')
vision_model = None
try:
    vision_model = load_model(MODEL_PATH)
    print("[1/2] ë¹„ì „ ëª¨ë¸(ë„ê°) ë¡œë“œ ì„±ê³µ!")
except Exception as e:
    print(f" ë„ê° ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

fish_names = [
    'ê°ì‹œë¶•ì–´', 'ê°ˆê²¨ë‹ˆ', 'ê°•ë„ë‹¤ë¦¬', 'ê³¤ë“¤ë§¤ê¸°', 'ê¸ˆê°•ëª¨ì¹˜',
    'êº½ì§€', 'ë…ì¤‘ê°œ', 'ëŒê³ ê¸°', 'ë¬´ì§€ê°œ ì†¡ì–´', 'ë¬µë‚©ìë£¨',
    'ë¯¸ìœ ê¸°', 'ë²„ë“¤ì¹˜', 'ë¸Œë¼ìš´ ì†¡ì–´', 'ë¹™ì–´', 'ì‚°ì²œì–´',
    'ì‰¬ë¦¬', 'ì—´ëª©ì–´', 'ì€ì–´', 'í°ê°€ì‹œê³ ê¸°', 'í™©ì–´'
]

# ==========================================
# [2] ë¬´ë£Œ RAG ì§€ì‹ë² ì´ìŠ¤ ë¡œë“œ (LCEL ìµœì‹  ê¸°ìˆ  ì ìš©)
# ==========================================
rag_chain = None
try:
    txt_path = os.path.join(BASE_DIR, 'fish_data.txt')
    loader = TextLoader(txt_path, encoding='utf-8')
    docs = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    splits = text_splitter.split_documents(docs)

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever()

    llm = ChatOllama(model="qwen2:0.5b")

    system_prompt = (
        "ë„ˆëŠ” ëƒ‰ìˆ˜ì–´ ì‚¬ìœ¡ ì „ë¬¸ê°€ì•¼. "
        "ë°˜ë“œì‹œ ì•„ë˜ì— ì œê³µëœ ë¬¸ì„œ(Context)ì˜ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•´ì„œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì¤˜. "
        "ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì´ë©´ 'ì œê³µëœ ë„ê° ì •ë³´ì—ëŠ” ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë§í•´.\n\n"
        "{context}"
    )
    prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{input}")])

    # ğŸ› ï¸ [ì‘ë™ ìˆœì„œ]: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ê¸´ ê¸€ë¡œ ì˜ˆì˜ê²Œ í•©ì³ì£¼ëŠ” ì—­í• ì…ë‹ˆë‹¤.
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # ğŸš€ [í•µì‹¬]: ì—ëŸ¬ ë‚˜ë˜ chains ëŒ€ì‹  ì‚¬ìš©í•˜ëŠ” ì§ê²° íŒŒì´í”„ë¼ì¸(LCEL)ì…ë‹ˆë‹¤.
    # [ì‘ë™ ìˆœì„œ]: ì§ˆë¬¸ ì…ë ¥ -> DB ê²€ìƒ‰ -> í”„ë¡¬í”„íŠ¸ ì¡°ë¦½ -> AI ë‹µë³€ -> í…ìŠ¤íŠ¸ë¡œ ê¹”ë”í•˜ê²Œ ì¶œë ¥
    rag_chain = (
        {"context": retriever | format_docs, "input": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    print("âœ… [2/2] 100% ë¬´ë£Œ ë¡œì»¬ RAG ì§€ì‹ë² ì´ìŠ¤ ì„¸íŒ… ì„±ê³µ! (LCEL ëª¨ë“œ)")

except Exception as e:
    print(f"âŒ RAG ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")


# ==========================================
# [3] ë¼ìš°í„° êµ¬ì—­
# ==========================================
@app.route('/api/ai/advice', methods=['POST'])
def get_fish_advice():
    data = request.json
    user_question = data.get('question')

    if not rag_chain:
        return jsonify({'status': 'fail', 'message': "AI ì§€ì‹ë² ì´ìŠ¤ ì—ëŸ¬"}), 500

    try:
        # LCEL ê¸°ìˆ ì„ ì“°ë©´ ë³µì¡í•˜ê²Œ ë”•ì…”ë„ˆë¦¬ì—ì„œ ë½‘ì•„ë‚¼ í•„ìš” ì—†ì´ ê³§ë°”ë¡œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ë‹µë³€ì´ ë‚˜ì˜µë‹ˆë‹¤.
        answer = rag_chain.invoke(user_question)
        return jsonify({'status': 'success', 'answer': answer})
    except Exception as e:
        return jsonify({'status': 'fail', 'message': f"ë¬´ë£Œ AI í†µì‹  ì—ëŸ¬: {str(e)}"}), 500


@app.route('/predict', methods=['POST'])
def predict():
    if 'file' not in request.files:
        return jsonify({"name": "ì—ëŸ¬", "description": "ì „ì†¡ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}), 400

    file = request.files['file']
    try:
        img = Image.open(io.BytesIO(file.read())).convert('RGB')
        img = img.resize((224, 224))
        img_array = np.array(img) / 255.0
        img_array = np.expand_dims(img_array, axis=0)

        prediction = vision_model.predict(img_array)
        confidence = np.max(prediction)
        result_index = np.argmax(prediction)

        if confidence < 0.6:
            return jsonify({"name": "ì¸ì‹ ë¶ˆê°€", "description": "ë¬¼ê³ ê¸° ì‚¬ì§„ì´ ì•„ë‹Œ ê²ƒ ê°™ìŠµë‹ˆë‹¤."})

        return jsonify({"name": fish_names[result_index], "description": f"í™•ë¥ : {confidence * 100:.1f}%"})
    except Exception as e:
        return jsonify({"name": "ì—ëŸ¬", "description": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"}), 500


@app.route('/api/custom_feature', methods=['POST', 'GET'])
def custom_feature():
    return jsonify({'status': 'success', 'message': 'ì—¬ë°± ê³µê°„'})


if __name__ == '__main__':
    print(" ê³¼ê¸ˆ ì—†ëŠ” 100% ë¡œì»¬ AI ì„œë²„ ê°€ë™ ì¤€ë¹„ ì™„ë£Œ (í¬íŠ¸: 5000)...")
    app.run(host='0.0.0.0', port=5000)